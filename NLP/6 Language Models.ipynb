{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NmJ45sd4DfJ"
      },
      "source": [
        "### Updating and checking the NLTK version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdqpY99J4DfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b252a343-ae41-48c1-b223-7cf8ed566a79"
      },
      "source": [
        "!pip install -U pip\n",
        "!pip install -U dill\n",
        "# !pip install -U nltk==3.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "Successfully installed pip-23.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (0.3.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4LRyLT4DfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c301f1-27ec-4abd-d524-022955ded87e"
      },
      "source": [
        "import nltk\n",
        "print(nltk.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNHLNx14Dfa"
      },
      "source": [
        "# N-gram using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFGS4Zqj4Dfc"
      },
      "source": [
        "Traditionally, we can use n-grams to generate language models to predict which word comes next given a history of words.\n",
        "\n",
        "We'll use the lm module in nltk to get a sense of how non-neural language modelling is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJPpMnL4Dfe"
      },
      "source": [
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "# from nltk.lm.preprocessing import flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx6Ugq5H4Dfg"
      },
      "source": [
        "If we want to train a bigram model, we need to turn this text into bigrams. Here's what the first sentence of our text would look like if we use the ngrams function from NLTK for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIxp6PEc4Dfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9efb74-070b-4b1e-f864-90650a23ee11"
      },
      "source": [
        "nltk.download('punkt')\n",
        "text = \"I am learning Text Analytics\"\n",
        "tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'learning', 'text', 'analytics']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jvjrMxgJmGq",
        "outputId": "f5de362c-917e-4a20-c8e5-26c14c87a6ee"
      },
      "source": [
        "list(bigrams(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'am'), ('am', 'learning'), ('learning', 'text'), ('text', 'analytics')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnPitK4x4Dfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f486a882-0243-4e2d-c425-2b6315415edf"
      },
      "source": [
        "list(ngrams(tokens, n=3)) # n = no of grams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'am', 'learning'),\n",
              " ('am', 'learning', 'text'),\n",
              " ('learning', 'text', 'analytics')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWdRUdqDKjJ9",
        "outputId": "18f17a3d-4ad1-4ac4-b3c8-64340bb14bb9"
      },
      "source": [
        "list(everygrams(tokens, max_len=3)) # max_len will set the no of maximum grams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i',),\n",
              " ('i', 'am'),\n",
              " ('i', 'am', 'learning'),\n",
              " ('am',),\n",
              " ('am', 'learning'),\n",
              " ('am', 'learning', 'text'),\n",
              " ('learning',),\n",
              " ('learning', 'text'),\n",
              " ('learning', 'text', 'analytics'),\n",
              " ('text',),\n",
              " ('text', 'analytics'),\n",
              " ('analytics',)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ3WIBre4Dfl"
      },
      "source": [
        "Add special \"padding\" symbols to the sentence before splitting it into ngrams. Fortunately, NLTK also has a function for that, let's see what it does to the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nYk84e4Dfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c6ed76-cf69-4f75-9cd3-4f06f2e0c6cc"
      },
      "source": [
        "from nltk.util import pad_sequence\n",
        "list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2)) \n",
        "# The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'i', 'am', 'learning', 'text', 'analytics', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JycGLGo44Dfp",
        "outputId": "b14c2bd5-1da0-449c-eec7-a09517be4417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "padded_sent = list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
        "list(ngrams(padded_sent, n=2)) # bigram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'i'),\n",
              " ('i', 'am'),\n",
              " ('am', 'learning'),\n",
              " ('learning', 'text'),\n",
              " ('text', 'analytics'),\n",
              " ('analytics', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFjCk4wr4Dfq"
      },
      "source": [
        "Note the n argument, that tells the function we need padding for bigrams.\n",
        "\n",
        "Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway.\n",
        "\n",
        "Thus the nltk.lm module provides a convenience function that has all these arguments already set while the other arguments remain the same as for pad_sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW1zDLJC4Dfs",
        "outputId": "ef0d63bd-3e72-4b7d-9d2b-ed80747a0fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "list(pad_both_ends(tokens, n=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '<s>', 'i', 'am', 'learning', 'text', 'analytics', '</s>', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHeRcmqT4Dfv"
      },
      "source": [
        "Combining the two parts discussed so far we get the following preparation steps for one sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoSVN5Pr4Dfw",
        "outputId": "9a8df4f5-5031-40c7-ed0d-ee20e282bbf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(bigrams(pad_both_ends(tokens, n=2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'i'),\n",
              " ('i', 'am'),\n",
              " ('am', 'learning'),\n",
              " ('learning', 'text'),\n",
              " ('text', 'analytics'),\n",
              " ('analytics', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaaxtRWA4Dfx"
      },
      "source": [
        "To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information. NLTK once again helpfully provides a function called everygrams.\n",
        "\n",
        "While not the most efficient, it is conceptually simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td3s76lE4Dfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d4bbad-4780-4c5c-dfd2-c851db0f8c4a"
      },
      "source": [
        "from nltk.util import everygrams\n",
        "padded_bigrams = list(pad_both_ends(tokens, n=2))\n",
        "list(everygrams(padded_bigrams, max_len=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>',),\n",
              " ('i',),\n",
              " ('am',),\n",
              " ('learning',),\n",
              " ('text',),\n",
              " ('analytics',),\n",
              " ('</s>',)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v55Up9lQ4Df0",
        "outputId": "c9044cad-b000-496e-9259-90fc7d20c647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(everygrams(padded_bigrams, max_len=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>',),\n",
              " ('<s>', 'i'),\n",
              " ('i',),\n",
              " ('i', 'am'),\n",
              " ('am',),\n",
              " ('am', 'learning'),\n",
              " ('learning',),\n",
              " ('learning', 'text'),\n",
              " ('text',),\n",
              " ('text', 'analytics'),\n",
              " ('analytics',),\n",
              " ('analytics', '</s>'),\n",
              " ('</s>',)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIjJOL0_4Df0"
      },
      "source": [
        "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model.\n",
        "\n",
        "To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUIJsAmK4Df1"
      },
      "source": [
        "### Calculating probability of n-grams in a text / sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ispXT0vT4Df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c9c807-2999-4e84-dec6-55f0a78694ed"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"I am learning Text Analytics\"\n",
        "# Tokenize the text.\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(text)))]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'am', 'learning', 'text', 'analytics']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUKOrmN4Df2"
      },
      "source": [
        "# Preprocess the tokenized text for 3-grams language modelling\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE # Maximum Likelihood Estimation\n",
        "\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model = MLE(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
        "model.fit(train_data, padded_sents) # model building"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WJHSaXLZVLF",
        "outputId": "0d25048d-a861-4b55-a72e-a0f672bba0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.lm.models.MLE at 0x7fedfc0bf850>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnYdiei54Df3"
      },
      "source": [
        "To get the counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1mkPaV4Df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef31fa0-c970-476d-c0ba-b5190d77d9da"
      },
      "source": [
        "model.counts['i'] # i.e. Count('i')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfarQJmg4Df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665ce7f8-426a-4d9b-a66d-9678185d1c72"
      },
      "source": [
        "model.counts[['i']]['am'] # i.e. Count('am'|'i')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-SbSr14Df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9735acd6-c116-4934-8771-a939d0265722"
      },
      "source": [
        "model.counts[['i', 'am']]['learning'] # i.e. Count('learning'|'i am')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the Probablity Values:"
      ],
      "metadata": {
        "id": "AhoVGb9JM6_2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldmdNkRT4Df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7cd807-88a8-4d15-85fc-15b94b88a37f"
      },
      "source": [
        "model.score('am', 'i'.split())  # P('am'|'i') = C(i am)/C(i) = 1/1 = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpMri5EG4Df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e892212e-ab7c-4c7c-a282-7b31b30414bf"
      },
      "source": [
        "model.score('learning', 'i am'.split())  # P('learning'|'i am')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHSROvTgc6QR",
        "outputId": "00a9fc4f-7814-4cec-9c70-010442cb60aa"
      },
      "source": [
        "len(model.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfdsJfj8br9g",
        "outputId": "d3bf0fab-1b0c-4994-d8b5-a4e4efc4e570"
      },
      "source": [
        "model.score(\"i\") # p(i) = c(i)/c(w)\n",
        "# tokens = 5 & pads = 4 ==> total = 9\n",
        "# c(i) = 1 & c(w) = 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNVz-kmRQzZb",
        "outputId": "74f0efc5-4cc6-4bef-c00a-9df01db56677"
      },
      "source": [
        "model.vocab.lookup(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('i', 'am', 'learning', 'text', 'analytics'),)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPAVz39gQ-is",
        "outputId": "66a8f416-9dc2-4f97-ef3b-99c2692c538b"
      },
      "source": [
        "model.vocab.lookup([\"i am playing\".split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('i', 'am', '<UNK>'),)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.counts[['i', 'am']]['playing'] # i.e. Count('playing'|'i am')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901Rt-fpmGe1",
        "outputId": "b0ce27e1-70d3-499b-a6a2-0b7a09c4022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplace Smoothing using NLTK**"
      ],
      "metadata": {
        "id": "M-Kzed3MRb45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model_laplace = Laplace(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
        "model_laplace.fit(train_data, padded_sents)"
      ],
      "metadata": {
        "id": "iLcXV-3bRfY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgjOsJIMSUTk",
        "outputId": "094b9263-ac34-4897-fc25-214c5a0cf004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.lm.models.Laplace at 0x7fedfc0b31f0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace.counts[['i']]['am']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NByJegp_STTF",
        "outputId": "2ee29aa1-e7ad-4564-94f0-814d2d62380b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace.score('am', 'i'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fNjiIrLSXbi",
        "outputId": "a06ffd75-8c8e-4aa7-e74c-f838c35d40cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2222222222222222"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM7Sdymv4Df7"
      },
      "source": [
        "## N-gram using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jd3lMAk4Df8",
        "outputId": "741d998a-a591-44b6-d46d-8107b48751a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        " \n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        " \n",
        "text = 'A class is a blueprint for the object.'\n",
        " \n",
        "print(\"1-gram: \", extract_ngrams(text, 1))\n",
        "print(\"2-gram: \", extract_ngrams(text, 2))\n",
        "print(\"3-gram: \", extract_ngrams(text, 3))\n",
        "print(\"4-gram: \", extract_ngrams(text, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZzeJq704Df-"
      },
      "source": [
        "## N-gram using TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v660htyc4Df-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e900a770-2eb0-49c3-8986-a607dfb25f72"
      },
      "source": [
        "from textblob import TextBlob\n",
        " \n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = TextBlob(data).ngrams(num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        " \n",
        "text = 'A class is a blueprint for the object.'\n",
        " \n",
        "print(\"1-gram: \", extract_ngrams(text, 1))\n",
        "print(\"2-gram: \", extract_ngrams(text, 2))\n",
        "print(\"3-gram: \", extract_ngrams(text, 3))\n",
        "print(\"4-gram: \", extract_ngrams(text, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOdF3spq4Df_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}